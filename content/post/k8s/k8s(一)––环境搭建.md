---
date :  "2020-01-28T13:36:27+08:00" 
title : "k8s(ä¸€)â€“â€“ç¯å¢ƒæ­å»º" 
categories : ["k8s"] 
tags : ["k8s"] 
toc : true
description : k8s ç¯å¢ƒæ­å»º
---

## K8Sç¯å¢ƒæ­å»º

### å…¶ä»–æ­å»ºæ–¹å¼

- å­¦ä¹ ç¯å¢ƒï¼Œå¯ä»¥å®‰è£… [minikube](https://kubernetes.io/docs/setup/learning-environment/minikube/#installation) ï¼ŒåŒæ—¶è¿˜æœ‰ä¸€ä¸ªåœ¨çº¿çš„ [minikube terminal](https://kubernetes.io/docs/tutorials/hello-minikube/#create-a-minikube-cluster)ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨[k8s kind](https://kind.sigs.k8s.io/) æ¥æ­å»ºç¯å¢ƒ
- ç”Ÿäº§ç¯å¢ƒï¼Œä½¿ç”¨ [kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/)ï¼Œæœ¬æ¬¡æ­å»ºä½¿ç”¨kubeadmæ¥æ­å»ºï¼Œæ›´å¤š [kubeadm command](https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm/)ã€‚

### Centos7ç¯å¢ƒæ­å»º

| IP              | è¯´æ˜   |
| --------------- | ------ |
| 192.168.128.220 | master |
| 192.168.128.221 | node1  |
| 192.168.128.222 | node2  |

> é…ç½®è¦æ±‚ï¼š>2C4G50G

### æ“ä½œç³»ç»Ÿè®¾ç½®

è®¾ç½®é™æ€IP

```shell
vi /etc/sysconfig/network-scripts/ifcfg-ens33

....
BOOTPROTO="static"
ONBOOT="yes"
.....
IPADDR=192.168.128.220
NETMASK=255.255.255.0
GATEWAY=192.168.128.2
```

é‡å¯

```shell
systemctl restart network
```

è®¾ç½®ä¸»æœºå

```shell
hostnamectl set-hostname k8s-master
```

è®¾ç½®åŸŸåï¼Œé…ç½®/ect/hosts

```shell
 cat >> /etc/hosts << EOF
192.168.128.220    k8s-master
192.168.128.221    k8s-node01
192.168.128.222    k8s-node02
EOF
```

å…³é—­é˜²ç«å¢™

```shell
systemctl status firewalld &  systemctl stop firewalld & systemctl status firewalld 
systemctl status iptables &  systemctl stop iptables & systemctl status iptables
```

å…³é—­selinux

```shell
setenforce 0

getenforce
vi /etc/selinux/config
```

å…³é—­swap

```shell
vi /etc/fstab
## æ³¨é‡Š
# /dev/mapper/centos-swap swap                    swap    defaults        0 0
```

å¼€å¯é€æ˜ç½‘æ¡¥

```shell
echo "net.bridge.bridge-nf-call-iptables=1" >> /etc/sysctl.d/k8s.conf
echo "net.bridge.bridge-nf-call-ip6tables=1" >> /etc/sysctl.d/k8s.conf
sysctl -p
```

å¼€å¯ipvs

```shell
cat > /etc/sysconfig/modules/ipvs.modules <<EOF
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF
```

```shell
#æ‰§è¡Œè„šæœ¬
chmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep -e ip_vs -e nf_conntrack_ipv4
```

### å®‰è£…docker-ce

```
yum remove docker-client docker-common docker -y
```

```shell
# step 1: å®‰è£…å¿…è¦çš„ä¸€äº›ç³»ç»Ÿå·¥å…·
sudo yum install -y yum-utils device-mapper-persistent-data lvm2
# Step 2: æ·»åŠ è½¯ä»¶æºä¿¡æ¯
sudo yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
# Step 3: æ›´æ–°å¹¶å®‰è£…Docker-CE
sudo yum makecache fast
sudo yum -y install docker-ce
```

é…ç½®cgroup

```shell
cat > /etc/docker/daemon.json <<EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"]
}
EOF
```

è®¾ç½®FORWARD ACCEPT

```shell
vi /usr/lib/systemd/system/docker.service
ExecStartPost=/usr/sbin/iptables -P FORWARD ACCEPT
```

é‡ç½®å¹¶è®¾ç½®å¼€æœºå¯åŠ¨

```shell
 systemctl daemon-reload && systemctl restart docker.service && systemctl enable docker.service
```

### å®‰è£…k8s

```shell
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
yum install -y kubelet kubeadm kubectl
```

å¯åŠ¨

```shell
systemctl enable kubelet && systemctl start kubelet
```

é€šè¿‡aliyunä»“åº“æ‹‰å–gc.src.ioçš„é•œåƒçš„ä¸€ä¸ªè„šæœ¬

```shell
vi /tmp/image.sh
#!/bin/bash
url=registry.cn-hangzhou.aliyuncs.com/google_containers
version=v1.17.2
images=(`kubeadm config images list --kubernetes-version=$version|awk -F '/' '{print $2}'`)
for imagename in ${images[@]} ; do
  docker pull $url/$imagename
  docker tag $url/$imagename k8s.gcr.io/$imagename
  docker rmi -f $url/$imagename
done

sh /tmp/image.sh
docker image
```

> versionè·å–ï¼škubelet --version

#### åˆå§‹åŒ–master

```shell
kubeadm init --image-repository registry.aliyuncs.com/google_containers  --kubernetes-version v1.17.0 --pod-network-cidr=10.244.0.0/16  --apiserver-advertise-address=192.168.128.220 
```

è¾“å‡ºä¿¡æ¯
```shell
kubeadm init --image-repository registry.aliyuncs.com/google_containers  --kubernetes-version v1.17.0 --pod-network-cidr=10.244.0.0/16  --apiserver-advertise-address=192.168.128.220 
W0203 12:06:56.185377    4249 validation.go:28] Cannot validate kube-proxy config - no validator is available
W0203 12:06:56.185442    4249 validation.go:28] Cannot validate kubelet config - no validator is available
[init] Using Kubernetes version: v1.17.0
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [k8s-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.128.220]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [k8s-master localhost] and IPs [192.168.128.220 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [k8s-master localhost] and IPs [192.168.128.220 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
W0203 12:07:53.278042    4249 manifests.go:214] the default kube-apiserver authorization-mode is "Node,RBAC"; using "Node,RBAC"
[control-plane] Creating static Pod manifest for "kube-scheduler"
W0203 12:07:53.279114    4249 manifests.go:214] the default kube-apiserver authorization-mode is "Node,RBAC"; using "Node,RBAC"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 20.032332 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.17" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node k8s-master as control-plane by adding the label "node-role.kubernetes.io/master=''"
[mark-control-plane] Marking the node k8s-master as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: j6crlr.k2eh15nkxw4ear9y
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.128.220:6443 --token j6crlr.k2eh15nkxw4ear9y \
    --discovery-token-ca-cert-hash sha256:b79e738df3cafd3d303707f877242cfb634429566c84e78818e86798be85f705
```

è®¾ç½®`kubectl`å‘½ä»¤è¡Œç¯å¢ƒ

```shell
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
```

æŸ¥çœ‹çŠ¶æ€

```shell
[root@k8s-master ~]# kubectl get nodes
NAME         STATUS     ROLES    AGE   VERSION
k8s-master   NotReady   master   36m   v1.17.2
```

çŠ¶æ€æœªæˆåŠŸçš„åŸå› æœªå®‰è£…ç½‘ç»œæ’ä»¶

```shell
kubectl describe node k8s-master | grep Ready
  Ready            False   Mon, 03 Feb 2020 12:48:44 +0800   Mon, 03 Feb 2020 12:08:03 +0800   KubeletNotReady              runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
```

å®‰è£…flannel

```shell
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
```

ç¦»çº¿å®‰è£…

```shell
curl -O https://github.com/coreos/flannel/blob/master/Documentation/kube-flannel-aliyun.yml
kubectl apply -f kube-flannel-aliyun.yml
```

æŸ¥çœ‹çŠ¶æ€

```shell
[root@k8s-master ~]# kubectl get nodes
NAME         STATUS   ROLES    AGE   VERSION
k8s-master   Ready    master   78m   v1.17.2
```

#### æ·»åŠ Node

```shell
kubeadm join 192.168.128.220:6443 --token j6crlr.k2eh15nkxw4ear9y --discovery-token-ca-cert-hash sha256:b79e738df3cafd3d303707f877242cfb634429566c84e78818e86798be85f705
```

```shell
[root@k8s-node1 ~]# kubectl get nodes
NAME         STATUS   ROLES    AGE     VERSION
k8s-master   Ready    master   4h29m   v1.17.2
k8s-node1    Ready    <none>   2m10s   v1.17.2
k8s-node2    Ready    <none>   14m     v1.17.2
```

### å¸è½½

- é‡ç½®

```shell
kubeadm reset
```

- åˆ é™¤ç›®å½•

```shell
rm -rf /etc/cni/
rm -rf /var/lib/etcd/
rm -rf /var/lib/kubelet/
rm -rf /var/lib/dockershim/
rm -rf /var/lib/cni
```

- æ¸…ç©ºiptablesè§„åˆ™

```shell
ipvsadm --clear
iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X
```

- åˆ é™¤ `ip link`ï¼Œåªéœ€è¦ä¿ç•™`lo,eth0,docker0`

```shell
ip link delete cni0
ip link delete flannel.1
ip link delete kube-ipvs0
```

### WebUI

```shell
kubectl proxy --address='0.0.0.0' --port=8001 --accept-hosts='.*'
```

[Tokenç”ŸæˆåŠç™»é™†](https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md)

### å¿…çœ‹èµ„æº

- [reference](https://kubernetes.io/docs/reference/)ï¼š å‘½ä»¤è¡Œå·¥å…·å‚æ•°ï¼Œåƒkubectlã€kubeadmç­‰ï¼Œè¿˜æœ‰ä¸€äº›APIåˆ—è¡¨
- [api-reference](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#container-v1-core):ç”¨äºæŸ¥æ‰¾ä¸€äº›é…ç½®å‚æ•°ä¿¡æ¯





## MacOSç¯å¢ƒæ­å»º(minikube)

### ä¸‹è½½Vm-Driver

macosä¸Šé¢å¯é€‰çš„`driver`æœ‰è®¸å¤šç§ [drivers/macos](https://minikube.sigs.k8s.io/docs/drivers/#macos)ï¼Œé€‰æ‹©ä¸€ä¸ªä¸‹è½½å³å¯ï¼›æˆ‘é€‰æ‹©çš„æ˜¯`virtualbox`

### å®‰è£…[Minikube](https://minikube.sigs.k8s.io/docs/start/)

ç‰ˆæœ¬è¦æ±‚åœ¨1.7ä»¥ä¸Šï¼Œå¦åˆ™åœ¨ä½¿ç”¨çš„æ—¶å€™ä¼šæŠ¥å¦‚ä¸‹é”™è¯¯ï¼š

```shell
âœ  ~ minikube start --vm-driver=virtualbox  --image-repository=https://registry.docker-cn.com --memory=4g
ğŸ˜„  minikube v1.6.2 on Darwin 10.14
âœ¨  Selecting 'virtualbox' driver from user configuration (alternates: [hyperkit])
âš ï¸  Not passing HTTP_PROXY=127.0.0.1:1087 to docker env.
âš ï¸  Not passing HTTPS_PROXY=127.0.0.1:1087 to docker env.
âœ…  Using image repository https://registry.docker-cn.com
ğŸ”¥  Creating virtualbox VM (CPUs=2, Memory=4000MB, Disk=20000MB) ...
panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x28 pc=0x4ecd70f]

goroutine 95 [running]:
github.com/google/go-containerregistry/pkg/v1/tarball.Write(0x0, 0xc00004c990, 0x6, 0xc00004c997, 0x1b, 0xc0002e40e3, 0x7, 0x0, 0x0, 0xc00053bc68, ...)
        /private/tmp/minikube-20191220-77113-wmp8w9/.brew_home/go/pkg/mod/github.com/google/go-containerregistry@v0.0.0-20180731221751-697ee0b3d46e/pkg/v1/tarball/write.go:57 +0x12f
k8s.io/minikube/pkg/minikube/machine.CacheImage(0xc0002e40c0, 0x2a, 0xc0002ee500, 0x4e, 0x0, 0x0)
        /private/tmp/minikube-20191220-77113-wmp8w9/pkg/minikube/machine/cache_images.go:395 +0x5df
k8s.io/minikube/pkg/minikube/machine.CacheImages.func1(0xc0003a5768, 0x0)
        /private/tmp/minikube-20191220-77113-wmp8w9/pkg/minikube/machine/cache_images.go:85 +0x124
golang.org/x/sync/errgroup.(*Group).Go.func1(0xc000498420, 0xc000498540)
        /private/tmp/minikube-20191220-77113-wmp8w9/.brew_home/go/pkg/mod/golang.org/x/sync@v0.0.0-20190423024810-112230192c58/errgroup/errgroup.go:57 +0x64
created by golang.org/x/sync/errgroup.(*Group).Go
        /private/tmp/minikube-20191220-77113-wmp8w9/.brew_home/go/pkg/mod/golang.org/x/sync@v0.0.0-20190423024810-112230192c58/errgroup/errgroup.go:54 +0x66
```

`minikube`çš„issuesé‡Œé¢ä¹Ÿæœ‰æè¿° [minikube/issues/6428](https://github.com/kubernetes/minikube/issues/6428)

#### brewå®‰è£…

```shell
brew install minikube
```

#### ç›´æ¥ä¸‹è½½å®‰è£…

```shell
curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-darwin-amd64
sudo install minikube-darwin-amd64 /usr/local/bin/minikube
```

### å®‰è£…é›†ç¾¤

```shell
minikube start --vm-driver=virtualbox
```

> ä¸­é—´ä¼šæ‹‰å–k8sçš„ç›¸å…³é•œåƒï¼Œéœ€è¦å¼€å¯proxyä»£ç†

### å®‰è£…kubectl

```shell
brew install kubectl
```

### æµ‹è¯•

éƒ¨ç½²nginx

```shell
kubectl apply  -f  https://k8s.io/examples/application/deployment.yaml
```

> éƒ¨ç½²å‰å¯ä»¥ä½¿ç”¨`kubectl get --watch deployment`è§‚å¯Ÿdeploymentçš„çŠ¶æ€

æŸ¥çœ‹çŠ¶æ€

```shell
âœ  ~ kubectl get deployments
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   0/2     2            0           4m30s
```

### ä¸€äº›ä¿¡æ¯

æŸ¥çœ‹é›†ç¾¤

```shell
âœ  ~ kubectl get po -A
NAMESPACE     NAME                               READY   STATUS    RESTARTS   AGE
kube-system   coredns-66bff467f8-nnm5b           1/1     Running   0          72m
kube-system   coredns-66bff467f8-wk9x5           1/1     Running   0          72m
kube-system   etcd-minikube                      1/1     Running   0          72m
kube-system   kube-apiserver-minikube            1/1     Running   0          72m
kube-system   kube-controller-manager-minikube   1/1     Running   0          72m
kube-system   kube-proxy-pxvlk                   1/1     Running   0          72m
kube-system   kube-scheduler-minikube            1/1     Running   0          72m
kube-system   storage-provisioner                1/1     Running   0          72m
```

æŸ¥çœ‹`Dashboard`

```shell
âœ  ~ minikube dashboard
ğŸ”Œ  Enabling dashboard ...
ğŸ¤”  Verifying dashboard health ...
ğŸš€  Launching proxy ...
ğŸ¤”  Verifying proxy health ...
ğŸ‰  Opening http://127.0.0.1:62758/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/ in your default browser...
```

æŸ¥çœ‹æœåŠ¡

```shell
âœ  ~ minikube addons list
|-----------------------------|----------|--------------|
|         ADDON NAME          | PROFILE  |    STATUS    |
|-----------------------------|----------|--------------|
| dashboard                   | minikube | enabled âœ…   |
| default-storageclass        | minikube | enabled âœ…   |
| efk                         | minikube | disabled     |
| freshpod                    | minikube | disabled     |
| gvisor                      | minikube | disabled     |
| helm-tiller                 | minikube | disabled     |
| ingress                     | minikube | disabled     |
| ingress-dns                 | minikube | disabled     |
| istio                       | minikube | disabled     |
| istio-provisioner           | minikube | disabled     |
| logviewer                   | minikube | disabled     |
| metrics-server              | minikube | disabled     |
| nvidia-driver-installer     | minikube | disabled     |
| nvidia-gpu-device-plugin    | minikube | disabled     |
| registry                    | minikube | disabled     |
| registry-aliases            | minikube | disabled     |
| registry-creds              | minikube | disabled     |
| storage-provisioner         | minikube | enabled âœ…   |
| storage-provisioner-gluster | minikube | disabled     |
|-----------------------------|----------|--------------|
```



### å¦‚ä½•è¿æ¥è‡³Minikube VM

#### æ–¹å¼ä¸€

```shell
âœ  ~ minikube ssh
                         _             _
            _         _ ( )           ( )
  ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __
/' _ ` _ `\| |/' _ `\| || , <  ( ) ( )| '_`\  /'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)

$
```

æŸ¥çœ‹IP

```shell
âœ  ~ minikube ip
192.168.99.102
```

ç”±äº`minikube`ä½¿ç”¨ [boot2docker](https://github.com/boot2docker/boot2docker#ssh-into-vm)ï¼Œæ‰€ä»¥é»˜è®¤ç”¨æˆ·åå¯†ç ä¸º`docker/tcuser`

```shell
ssh docker@192.168.99.102
```

